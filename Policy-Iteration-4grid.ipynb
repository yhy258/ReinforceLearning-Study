{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReinforcementLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER66n-8X5aIZ"
      },
      "source": [
        "# Planning - Dynamic Programming\n",
        "https://www.youtube.com/watch?v=dw0sHzE1oAc&t=3278s \\\\\n",
        "Naver d2 \"Introduction of Deep Reinforcement Learning\" 를 참고했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCubn9sk5noh"
      },
      "source": [
        "## Policy Iteration\n",
        "### 4x4 grid, reward : all -1, action : left, right, up, down"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub0PiGn75nKE",
        "outputId": "e0851473-0d7c-4e12-8181-fb07e638bca0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "    0, 0 시작\n",
        "    index상으로는 0 미만,  3 초과일 수 없음.\n",
        "\"\"\"\n",
        "\n",
        "state = [0, 0] \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Function _ get_state \n",
        "    : 현재 state에서 action에 의해 변하는 것.\n",
        "\"\"\"\n",
        "def get_state(state, action): # action , 0: Up, 1 : Down, 2 : Left, 3 : Right\n",
        "    action_list = [[-1, 0], [1, 0], [0,-1], [0, 1]]\n",
        "    state[0] += action_list[action][0]\n",
        "    state[1] += action_list[action][1]\n",
        "    if state[0] < 0:\n",
        "        state[0] = 0\n",
        "    if state[0] > 3 :\n",
        "        state[0] = 3\n",
        "    if state[1] < 0:\n",
        "        state[1] = 0\n",
        "    if state[1] > 3:\n",
        "        state[1] = 3\n",
        "\n",
        "    return state[0], state[1]\n",
        "\n",
        "\"\"\"\n",
        "    Function _ policy_eval\n",
        "    : iterative evaluation 부분.\n",
        "\n",
        "    grid_values는 grid형태 (4 x 4) 의 values table\n",
        "    \n",
        "    value table은 그 해당 위치 (state)에서의 미래 상황을 살펴본다. one step or multi step\n",
        "\n",
        "    action : {0: Up, 1 : Down, 2 : Left, 3 : Right}\n",
        "    policy : Prob of each states, each action \n",
        "    reward : Reward value of each states, each action\n",
        "\"\"\"\n",
        "\n",
        "def policy_eval(grid_width, grid_height, policy, discount_factor, iters, reward):\n",
        "    # Function get_state에 정의되어있는 action_list에 index로서 들어간다.\n",
        "    action = [0, 1, 2 ,3]\n",
        "\n",
        "    # 다음 Iteration에서의 value를 구하는데 이전 Iteration의 value값이 필요함.\n",
        "    prev_value_table = np.zeros([grid_height, grid_width]) \n",
        "\n",
        "    # 인자로서 받은 iteration만큼 돌려준다. (많이 돌려서 어느 값에 수렴하는 지 확인 -> true value)\n",
        "    for iter in range(1, iters+1):\n",
        "        # each iteration에서 각 state의 value를 넣어 줄 수 있는 value_table 구성\n",
        "        next_value_table = np.zeros([grid_height, grid_width])\n",
        "        # 모든 state를 모두 한 번씩 조회해보자.\n",
        "        for i in range(grid_height):\n",
        "            for j in range(grid_width):\n",
        "                # State가 시작점 혹은 도착점이면 해당 state는 넘긴다.\n",
        "                if (i == j) and ((i == 0) or (i == 3)) :\n",
        "                    continue \n",
        "\n",
        "                # 가능한 action들을 모두 고려해서 sum해줘야 해서 action을 통해 for문 돌려준다.\n",
        "                t_val = 0\n",
        "                for act in action :\n",
        "                    i_, j_ = get_state([i, j], act)\n",
        "                    val = policy[i][j][act]*(reward[i][j][act] + discount_factor * prev_value_table[i_][j_])\n",
        "            \n",
        "                    t_val += val\n",
        "\n",
        "                next_value_table[i][j] = round(t_val,3)\n",
        "                \n",
        "        prev_value_table = next_value_table\n",
        "    return next_value_table\n",
        "\n",
        "grid_width = 4\n",
        "grid_height = 4\n",
        "action = 4  # action : {0: Up, 1 : Down, 2 : Left, 3 : Right}\n",
        "policy = np.zeros([grid_height, grid_width, action])\n",
        "reward = np.zeros([grid_height, grid_width, action])\n",
        "for i in range(grid_height):\n",
        "    for j in range(grid_width):\n",
        "        for a in range(action):\n",
        "            policy[i][j][a] = 0.25\n",
        "            reward[i][j][a] = -1\n",
        "\n",
        "\n",
        "\n",
        "for iter in range(10, 101, 10):\n",
        "    value_table = policy_eval(grid_width, grid_height, policy, discount_factor= 1., iters = iter, reward = reward)\n",
        "    print(\"NOW ITERATION : \", iter)\n",
        "    print(value_table)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "print(\"\"\"\n",
        "    계속 iteration을 높이다 보면 어느 값에 수렴하는 것을 알 수 있다.\n",
        "    이를 true value라고 함.\n",
        "\"\"\")\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NOW ITERATION :  10\n",
            "[[ 0.    -6.138 -8.352 -8.968]\n",
            " [-6.138 -7.737 -8.428 -8.352]\n",
            " [-8.352 -8.428 -7.737 -6.138]\n",
            " [-8.968 -8.352 -6.138  0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  20\n",
            "[[  0.     -9.45  -13.257 -14.454]\n",
            " [ -9.45  -12.06  -13.302 -13.257]\n",
            " [-13.257 -13.302 -12.06   -9.45 ]\n",
            " [-14.454 -13.257  -9.45    0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  30\n",
            "[[  0.    -11.366 -16.096 -17.632]\n",
            " [-11.366 -14.562 -16.123 -16.097]\n",
            " [-16.096 -16.123 -14.562 -11.366]\n",
            " [-17.632 -16.097 -11.366   0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  40\n",
            "[[  0.    -12.475 -17.74  -19.471]\n",
            " [-12.475 -16.01  -17.755 -17.74 ]\n",
            " [-17.74  -17.755 -16.01  -12.475]\n",
            " [-19.471 -17.74  -12.475   0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  50\n",
            "[[  0.    -13.117 -18.691 -20.536]\n",
            " [-13.117 -16.847 -18.7   -18.691]\n",
            " [-18.691 -18.7   -16.847 -13.117]\n",
            " [-20.536 -18.691 -13.117   0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  60\n",
            "[[  0.    -13.489 -19.242 -21.152]\n",
            " [-13.489 -17.333 -19.248 -19.242]\n",
            " [-19.242 -19.248 -17.333 -13.489]\n",
            " [-21.152 -19.242 -13.489   0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  70\n",
            "[[  0.    -13.704 -19.562 -21.51 ]\n",
            " [-13.704 -17.614 -19.564 -19.562]\n",
            " [-19.562 -19.564 -17.614 -13.704]\n",
            " [-21.51  -19.562 -13.704   0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  80\n",
            "[[  0.    -13.829 -19.746 -21.716]\n",
            " [-13.829 -17.777 -19.748 -19.746]\n",
            " [-19.746 -19.748 -17.777 -13.829]\n",
            " [-21.716 -19.746 -13.829   0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  90\n",
            "[[  0.    -13.9   -19.853 -21.835]\n",
            " [-13.901 -17.87  -19.854 -19.853]\n",
            " [-19.853 -19.854 -17.87  -13.901]\n",
            " [-21.835 -19.853 -13.9     0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "NOW ITERATION :  100\n",
            "[[  0.    -13.942 -19.915 -21.905]\n",
            " [-13.942 -17.925 -19.916 -19.915]\n",
            " [-19.915 -19.916 -17.925 -13.942]\n",
            " [-21.905 -19.915 -13.942   0.   ]]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    계속 iteration을 높이다 보면 어느 값에 수렴하는 것을 알 수 있다.\n",
            "    이를 true value라고 함.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yeGfehRE5Wt"
      },
      "source": [
        "## Policy Improvement\n",
        "### Greedy Policy Improvement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG6Fk9dJ9ZNl",
        "outputId": "df23b734-252c-456e-c765-9c1a734173b6"
      },
      "source": [
        "\"\"\"\n",
        "    <Action Table>\n",
        "    0  1  2  3\n",
        "    4  5  6  7\n",
        "    8  9  10 11\n",
        "    12 13 14 15\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def greedy_policy(value_table, grid_width, grid_height, action_num):\n",
        "    action_table = np.zeros([grid_height, grid_width]) # action이 여러개 존재 할 수는 있는데, 하나만 넣자\n",
        "    action = [0, 1, 2, 3]\n",
        "    # Greedy 하게 정할 거다. 해당 state를 기준으로 어디로 가는게 제일 효율적인가 를 본다.\n",
        "    for i in range(grid_height):\n",
        "        for j in range(grid_width):\n",
        "            max_value = -100\n",
        "            for act in action : \n",
        "                i_, j_ = get_state([i,j], act)\n",
        "                value = value_table[i_][j_]\n",
        "                if max_value < value :\n",
        "                    arg_ = act\n",
        "                    max_value = value\n",
        "            action_table[i,j] = arg_\n",
        "    return action_table\n",
        "\n",
        "action_table = greedy_policy(value_table, 4,4,4)\n",
        "\n",
        "\n",
        "\n",
        "action_dict = {0: \"Up\", 1 :\"Down\", 2 : \"Left\", 3 : \"Right\"}\n",
        "action_str_table = []\n",
        "for i in range(4):\n",
        "    action_str = []\n",
        "    for j in range(4):\n",
        "        if (i == j) and ((i == 0) or (i == 3)):\n",
        "            action_str.append('T')    \n",
        "        else :\n",
        "            action_str.append(action_dict[action_table[i][j]])\n",
        "    action_str_table.append(action_str)\n",
        "\n",
        "print(\"Greedy Policy Improvement\")\n",
        "action_str_table\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Greedy Policy Improvement\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['T', 'Left', 'Left', 'Down'],\n",
              " ['Up', 'Up', 'Down', 'Down'],\n",
              " ['Up', 'Up', 'Down', 'Down'],\n",
              " ['Up', 'Right', 'Right', 'T']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}